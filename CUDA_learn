#2022.07.05
参考链接：
        1.  https://zhuanlan.zhihu.com/p/34587739，CUDA编程入门极简教程。
        2. https://blog.csdn.net/u013025612/article/details/47039437，左移（1<<20）。
        3. https://developer.nvidia.com/blog/even-easier-introduction-cuda/，An Even Easier Introduction to CUDA。
        
1. cuda的基本概念
        1.1. 一般说的GPU运算指的是基于CPU+GPU的异构计算。在异构计算中，GPU与CPU需要通过PCIe总线连接在一起来工作，CPU所在位置称为主机端（host），
                GPU所在位置称为设备端（device）。
        1.2. GPU运算核心多，适合大规模并行计算，但每个核心的性能较弱。
                CPU运算核心相对较少，但每个核心性能较强，可以实现复杂的逻辑运算。
                综合而言，两者能力互补，CPU负责处理逻辑复杂的串行程序，GPU负责重点处理数据密集型的并行计算程序。
        1.3. CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语
                言的支持，如C/C++，Python，Fortran等语言。
        1.4. 在CUDA程序中，host和device是两个重要的概念。一般而言，host指代CPU及其内存，device指代GPU及其内存。CUDA程序中既包含host程序，又包含
                device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：
                - 分配host内存，并进行数据初始化；
                - 分配device内存，并从host中将数据拷贝到device上；
                - 调用CUDA的核函数（kernel）在device上完成指定的运算；
                - 将device上的运算结果拷贝到host上；
                - 释放device和host上分配的内存。
        1.5. kernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用__global__符号声明，在调用时需要用<<<grid, block>>>来指定
                kernel要执行的线程数量。在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变
                量threadIdx来获得。
        1.6. 由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型
                限定词如下：
                - __global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员
                  函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。
                - __device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用。
                - __host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。                                        
        1.7. 软件概念层面：
                        -   一个kernel所启动的所有线程称为一个网格（grid），一个grid中可以包含一个或多个线程块（block），一个block中包含很多个线程。
                硬件概念层面：
                        -   一个Device包含一个或多个Streaming Multiprocessor（SM），一个SM包含一个或多个CUDA Core（SM的执行单元是线程束，一个线程束包含32个线
                            程，因此block的数量一般设置为32的的倍数）。
                        -   SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。
                        -   当一个kernel被执行时，它的gird中的线程块被分配到SM上，一个线程块只能在一个SM上被调度。SM一般可以调度多个线程块，这要看SM本身的能力。
                        -   SM采用的是SIMT (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是线程束（warps)，线程束包含32个线程，这些线程同时
                            执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。
                总之，就是网格和线程块只是逻辑划分，一个kernel的所有线程其实在物理层是不一定同时并发的。所以kernel的grid和block的配置不同，性能会出现差异，这
                点是要特别注意的。
        1.8. grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block
                可以灵活地定义为1-dim，2-dim以及3-dim结构。kernel在调用时也必须通过执行配置<<<grid, block>>>来指定kernel所使用的线程数及结构。
                例子：
                        dim3 grid(3, 2);
                        dim3 block(5, 3);
                        kernel_fun<<< grid, block >>>(prams...);
        1.9. 一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识，它们都是dim3类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线
               程所在block中的位置。
               blockDim来获取线程块各个维度的大小。
               另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。
        1.10. CUDA的内存模型，如下图所示。可以看到，每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线
                程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）
                和纹理内存（Texture Memory）。内存结构涉及到程序优化。
2. cuda基本程序学习
        2.1. 加法
                // 两个向量加法kernel，grid和block均为一维
                __global__ void add(float* x, float * y, float* z, int n)
                {
                    // 获取全局索引
                    int index = threadIdx.x + blockIdx.x * blockDim.x;
                   // 步长
                    int stride = blockDim.x * gridDim.x;
                    for (int i = index; i < n; i += stride)
                    {
                        z[i] = x[i] + y[i];
                    }
                }
                int main()
                {
                    int N = 1 << 20;
                    int nBytes = N * sizeof(float);

                    // 申请托管内存
                    float *x, *y, *z;
                    cudaMallocManaged((void**)&x, nBytes);
                    cudaMallocManaged((void**)&y, nBytes);
                    cudaMallocManaged((void**)&z, nBytes);

                    // 初始化数据
                    for (int i = 0; i < N; ++i)
                    {
                        x[i] = 10.0;
                        y[i] = 20.0;
                    }

                    // 定义kernel的执行配置
                    dim3 blockSize(256);
                    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
                    // 执行kernel
                    add << < gridSize, blockSize >> >(x, y, z, N);

                    // 同步device 保证结果能正确访问
                    cudaDeviceSynchronize();
                    // 检查执行结果
                    float maxError = 0.0;
                    for (int i = 0; i < N; i++)
                        maxError = fmax(maxError, fabs(z[i] - 30.0));
                    std::cout << "最大误差: " << maxError << std::endl;

                    // 释放内存
                    cudaFree(x);
                    cudaFree(y);
                    cudaFree(z);

                    return 0;
                }
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.06
参考链接：
        1. https://zhuanlan.zhihu.com/p/360897341，CUDA 编程小练习（一）。
        2. https://medium.com/@iphoenix179/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772，Running CUDA C/C++ 
             in Jupyter or how to run nvcc in Google CoLab。
        3. https://developer.nvidia.com/blog/even-easier-introduction-cuda/， An Even Easier Introduction to CUDA。
        
1. cuda环境搭建
    由于没有GPU，根据 https://zhuanlan.zhihu.com/p/360897341 中的建议采用colab进行cuda基本的程序练习。
    1.1. 打开colab，建立一个新的笔记本（然后安需要修改笔记本名字）。
    1.2. 点击代码执行程序（Runtimes）-> 选择更改运行时类型（change runtime type）->  硬件加速器（Hardware accelerator）选择GPU。
    1.3. 然后，需要安装juputer的一个插件以支持NVCC的源码编译。此部分内容与原链接中描述的稍有不同，因为在我的尝试中，原文的代码
            无法访问github（网页可以的登陆，初步排除vpn的问题）。
            - 原代码：
                    !pip install git+git://github.com/depctg/nvcc4jupyter.git
                    %load_ext nvcc_plugin
                    !nvcc --version
            - 修改过的可运行代码：
                    !pip install git+https://github.com/depctg/nvcc4jupyter.git          //将原来的"git+git:" 改为了"git+https:"
                    %load_ext nvcc_plugin
                    !nvcc --version
    1.4. 测试例程（此程序也来自与此链接）
            %%cu                                                                                                                             //当程序的语言是c/c++时，需要告诉interpreter，此处的%cu就是这个作用。
            #include <stdio.h>
            __global__ void cuda_hello()
            {
                    printf("Hello world from GPU!\n");
            }
            
            int main()
            {
                    cuda_hello<<<1,1>>>();
                    cudaDeviceSynchronize();                                                                           //由于__global__形式的函数是异步的，因此需要同步一下（除此之外需要这个函数才可以将printf显示到屏幕上）。
                                                                                                                                                    //Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because 
                                                                                                                                                    //CUDA kernel launches don’t block the calling CPU thread). To do this I just call cudaDeviceSynchronize() 
                                                                                                                                                    //before doing the final error checking on the CPU.
                    return 0;
            }
    1.5. 在colab测试程序:
                ------------------------------------------------------------------------------------------
                //一个block，一个thread：
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          for (int i = 0; i < n; i++)
                            y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                //一个block，多个thread：
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          int index = threadIdx.x;
                          int stride = blockDim.x;
                          for (int i = index; i < n; i += stride)
                              y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                //多个block，多个thread：                
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          int index = blockIdx.x * blockDim.x + threadIdx.x;
                          int stride = blockDim.x * gridDim.x;
                          for (int i = index; i < n; i += stride)
                            y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                 //主函数
                 int main(void)
                {
                  int N = 1<<20;
                  float *x, *y;

                  cudaMallocManaged(&x, N*sizeof(float));
                  cudaMallocManaged(&y, N*sizeof(float));

                  for (int i = 0; i < N; i++) {
                    x[i] = 1.0f;
                    y[i] = 2.0f;
                  }
                ------------------------------------------------------------------------------------------
                //一个block，一个thread
                 add<<<1, 1>>>(N, x, y); 
                ------------------------------------------------------------------------------------------
                 //一个block，多个thread
                 add<<<1, 256>>>(N, x, y); 
                ------------------------------------------------------------------------------------------
                //多个block，多个thread
                int blockDim=256; 
                int gridNum=(N+blockDim-1)/blockDim;
                add<<<gridNum, blockDim>>>(N, x, y);
                ------------------------------------------------------------------------------------------
                  cudaDeviceSynchronize();

                  float maxError = 0.0f;
                  for (int i = 0; i < N; i++)
                    maxError = fmax(maxError, fabs(y[i]-3.0f));
                  std::cout << "Max error: " << maxError << std::endl;

                  cudaFree(x);
                  cudaFree(y);

                  return 0;
                }
    1.6. At its core are three key abstractions 
                - a hierarchy of thread groups, 
                - shared memories, 
                - barrier synchronization 
            that are simply exposed to the programmer as a minimal set of language extensions.
            These abstractions provide：
                - fine-grained data parallelism，
                - thread parallelism, 
                - nested within coarse-grained data parallelism，
                - task parallelism.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.07
参考链接：
        1. https://zhuanlan.zhihu.com/p/360897341，CUDA 编程小练习（一），使用colab进行cuda学习。
        2. https://zhuanlan.zhihu.com/p/361180950，CUDA 编程小练习（二），向量的加法。
        3. https://zhuanlan.zhihu.com/p/361467248，CUDA 编程小练习（三），向量的内积（单线程）。
        4. https://zhuanlan.zhihu.com/p/361997521，CUDA 编程小练习（四），矩阵加法。
        5. https://zhuanlan.zhihu.com/p/362796754，CUDA 编程小练习（五），使用nvprof来测试并优化程序。
        6. https://zhuanlan.zhihu.com/p/363820880，CUDA 编程小练习（六），多线程块向量加法。
        7. https://zhuanlan.zhihu.com/p/366196428，CUDA 编程小练习（七），单block多线程向量乘法。
        8. 
1. 简单程序练习
    此部分代码的书写逻辑均按照：
            (1) 分配cpu内存：e.g.  (float*)malloc(sizeof(float)*vectorLength);
            (2) 初始化被运算数据；
            (3) 分配gpu内存：e.g. cudaMalloc((void**)&x, sizeof(float)*length);
            (4) 拷贝运算数据从cpu内存到gpu内存，e.g.  cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);
            (5) gpu运算（使用kernel函数）；
            (6) 从gpu内存中将计算结果提取出来转移到cpu内存中，cudaMemcpy(result, d_result, sizeof(float),  cudaMemcpyDeviceToHost);
            (7) 对运算结果的数据处理；
            (8) 释放cpu和gpu用到的内存。
    1.1. 向量加法。
    1.2. 向量内积（单线程）。
    1.3. 矩阵加法。
    1.4. 使用nvprof来测试可执行程序的运行效率，并可根据结果优化程序。
    1.5. 使用单block多线程计算向量内积。
                解决思路：
                        1. 单block多线程算乘法，然后单线程算加法。
                        2. 单block多线程算乘法和本线程内的加法，然后单线程求和。
                - 例子程序复现没有问题，新增知识点：
                        extern __shared__ float temp[];  //  定义了一块block内，线程之间可以共享的内存块，其长度在调用CUDA函数的时候会被动态划分，也正因此，
                                                                                                上面这个函数可以实现任意长度的向量内积；缺点是temp的长度与输入向量长度相等，如果GPU内同时有大
                                                                                                量的向量在做点乘，就会比较耗费显存；
                        __syncthreads(); //  用于线程间的同步，在做累加之前需要确保每个线程都已经完成了乘法。                                                                      
                - 在改写过程中出现问题，我使用cudaMallocManaged()函数替代了原程序的内存操作步骤，但运算结果一直为0（前面改动后也一直为0），原因待查。
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.08
参考链接：
        1. https://zhuanlan.zhihu.com/p/366196428，CUDA 编程小练习（七），向量内积。
        2. https://zhuanlan.zhihu.com/p/367833053，CUDA 编程小练习（八），矩阵乘法。
        3. https://developer.nvidia.com/tensorrt-getting-started，TensorRT Getting Started。
        4. https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt-updated/, Speeding Up Deep Learning Inference Using NVIDIA TensorRT (Updated)。
        
    1.1. （问题尚未解决）在colab继续昨天的问题分析，问题查询思路：
                        - 根据手动计算，确定运算节点的位置并输出当前节点的计算结果，以此来判断改写出错在哪里。
                            - __global__ 函数中的中间参数无法打印输出，原因待分析。
                            - 经测试，使用cudaMallocManaged()改写后的数据中（一共有result, v1, v2, vLength)，v1和v2的数据是对的，但result的数据是不符合预期的（依然为0）。
                            - 重写了一边进行逻辑上查验，还是没有发现问题。
    1.2. 参考2的函数：（应该有优化空间，例如temp[]）
            __global__ void matrix_vector_multiplication(float* vector_result, float *matrix_a, float *vector_b, int m_row, int n_col) 
            {
                extern __shared__ float temp[]; 
                int tid = blockIdx.x * blockDim.x + threadIdx.x;

                // 上面这一生成索引会有tid超出矩阵边界的风险，因此需要用下面的条件语句加限制
                int size_of_the_matrix = m_row*n_col;
                if(tid<size_of_the_matrix)
                {
                    temp[tid] = matrix_a[tid] * vector_b[threadIdx.x]; // 所有的乘法，近乎同时完成。
                }

                __syncthreads(); // synchronize all threads

                if (threadIdx.x == 0)
                {
                    float sum = 0;
                    int index = blockIdx.x * blockDim.x;
                    for (int i = index; i < index + blockDim.x ; i++)
                    {
                        sum += temp[i];
                    }
                    vector_result[blockIdx.x] = sum;
                }
        }
2. TensorRT基本概念
    2.1.  NVIDIA TensorRT是深度学习的推理SDK（Software Development Kit，软件开发工具包）。
            - TensorRT provides APIs and parsers to import trained models from all major deep learning frameworks.
            - It then generates optimized runtime engines deployable in the datacenter as well as in automotive and embedded environments.
    2.2. 介绍的例子：
            概述：It uses a C++ example to walk you through converting a PyTorch model into an ONNX model and importing it into TensorRT, 
                         applying optimizations, and generating a high-performance runtime engine for the datacenter environment.
            具体任务：
                - deploy a deep learning application onto a GPU;
                - increasing throughput;
                - reducing latency during inference.
    

