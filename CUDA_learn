#2022.07.05
参考链接：
        1.  https://zhuanlan.zhihu.com/p/34587739，CUDA编程入门极简教程。
        2. https://blog.csdn.net/u013025612/article/details/47039437，左移（1<<20）。
        3. https://developer.nvidia.com/blog/even-easier-introduction-cuda/，An Even Easier Introduction to CUDA。
        
1. cuda的基本概念
        1.1. 一般说的GPU运算指的是基于CPU+GPU的异构计算。在异构计算中，GPU与CPU需要通过PCIe总线连接在一起来工作，CPU所在位置称为主机端（host），
                GPU所在位置称为设备端（device）。
        1.2. GPU运算核心多，适合大规模并行计算，但每个核心的性能较弱。
                CPU运算核心相对较少，但每个核心性能较强，可以实现复杂的逻辑运算。
                综合而言，两者能力互补，CPU负责处理逻辑复杂的串行程序，GPU负责重点处理数据密集型的并行计算程序。
        1.3. CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语
                言的支持，如C/C++，Python，Fortran等语言。
        1.4. 在CUDA程序中，host和device是两个重要的概念。一般而言，host指代CPU及其内存，device指代GPU及其内存。CUDA程序中既包含host程序，又包含
                device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：
                - 分配host内存，并进行数据初始化；
                - 分配device内存，并从host中将数据拷贝到device上；
                - 调用CUDA的核函数（kernel）在device上完成指定的运算；
                - 将device上的运算结果拷贝到host上；
                - 释放device和host上分配的内存。
        1.5. kernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用__global__符号声明，在调用时需要用<<<grid, block>>>来指定
                kernel要执行的线程数量。在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变
                量threadIdx来获得。
        1.6. 由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型
                限定词如下：
                - __global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员
                  函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。
                - __device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用。
                - __host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。                                        
        1.7. 软件概念层面：
                        -   一个kernel所启动的所有线程称为一个网格（grid），一个grid中可以包含一个或多个线程块（block），一个block中包含很多个线程。
                硬件概念层面：
                        -   一个Device包含一个或多个Streaming Multiprocessor（SM），一个SM包含一个或多个CUDA Core（SM的执行单元是线程束，一个线程束包含32个线
                            程，因此block的数量一般设置为32的的倍数）。
                        -   SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。
                        -   当一个kernel被执行时，它的gird中的线程块被分配到SM上，一个线程块只能在一个SM上被调度。SM一般可以调度多个线程块，这要看SM本身的能力。
                        -   SM采用的是SIMT (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是线程束（warps)，线程束包含32个线程，这些线程同时
                            执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。
                总之，就是网格和线程块只是逻辑划分，一个kernel的所有线程其实在物理层是不一定同时并发的。所以kernel的grid和block的配置不同，性能会出现差异，这
                点是要特别注意的。
        1.8. grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block
                可以灵活地定义为1-dim，2-dim以及3-dim结构。kernel在调用时也必须通过执行配置<<<grid, block>>>来指定kernel所使用的线程数及结构。
                例子：
                        dim3 grid(3, 2);
                        dim3 block(5, 3);
                        kernel_fun<<< grid, block >>>(prams...);
        1.9. 一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识，它们都是dim3类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线
               程所在block中的位置。
               blockDim来获取线程块各个维度的大小。
               另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。
        1.10. CUDA的内存模型，如下图所示。可以看到，每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线
                程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）
                和纹理内存（Texture Memory）。内存结构涉及到程序优化。
2. cuda基本程序学习
        2.1. 加法
                // 两个向量加法kernel，grid和block均为一维
                __global__ void add(float* x, float * y, float* z, int n)
                {
                    // 获取全局索引
                    int index = threadIdx.x + blockIdx.x * blockDim.x;
                   // 步长
                    int stride = blockDim.x * gridDim.x;
                    for (int i = index; i < n; i += stride)
                    {
                        z[i] = x[i] + y[i];
                    }
                }
                int main()
                {
                    int N = 1 << 20;
                    int nBytes = N * sizeof(float);

                    // 申请托管内存
                    float *x, *y, *z;
                    cudaMallocManaged((void**)&x, nBytes);
                    cudaMallocManaged((void**)&y, nBytes);
                    cudaMallocManaged((void**)&z, nBytes);

                    // 初始化数据
                    for (int i = 0; i < N; ++i)
                    {
                        x[i] = 10.0;
                        y[i] = 20.0;
                    }

                    // 定义kernel的执行配置
                    dim3 blockSize(256);
                    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
                    // 执行kernel
                    add << < gridSize, blockSize >> >(x, y, z, N);

                    // 同步device 保证结果能正确访问
                    cudaDeviceSynchronize();
                    // 检查执行结果
                    float maxError = 0.0;
                    for (int i = 0; i < N; i++)
                        maxError = fmax(maxError, fabs(z[i] - 30.0));
                    std::cout << "最大误差: " << maxError << std::endl;

                    // 释放内存
                    cudaFree(x);
                    cudaFree(y);
                    cudaFree(z);

                    return 0;
                }
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.06
参考链接：
        1. https://zhuanlan.zhihu.com/p/360897341，CUDA 编程小练习（一）。
        2. https://medium.com/@iphoenix179/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772，Running CUDA C/C++ 
             in Jupyter or how to run nvcc in Google CoLab。
        3. https://developer.nvidia.com/blog/even-easier-introduction-cuda/， An Even Easier Introduction to CUDA。
        
1. cuda环境搭建
    由于没有GPU，根据 https://zhuanlan.zhihu.com/p/360897341 中的建议采用colab进行cuda基本的程序练习。
    1.1. 打开colab，建立一个新的笔记本（然后安需要修改笔记本名字）。
    1.2. 点击代码执行程序（Runtimes）-> 选择更改运行时类型（change runtime type）->  硬件加速器（Hardware accelerator）选择GPU。
    1.3. 然后，需要安装juputer的一个插件以支持NVCC的源码编译。此部分内容与原链接中描述的稍有不同，因为在我的尝试中，原文的代码
            无法访问github（网页可以的登陆，初步排除vpn的问题）。
            - 原代码：
                    !pip install git+git://github.com/depctg/nvcc4jupyter.git
                    %load_ext nvcc_plugin
                    !nvcc --version
            - 修改过的可运行代码：
                    !pip install git+https://github.com/depctg/nvcc4jupyter.git          //将原来的"git+git:" 改为了"git+https:"
                    %load_ext nvcc_plugin
                    !nvcc --version
    1.4. 测试例程（此程序也来自与此链接）
            %%cu                                                                                                                             //当程序的语言是c/c++时，需要告诉interpreter，此处的%cu就是这个作用。
            #include <stdio.h>
            __global__ void cuda_hello()
            {
                    printf("Hello world from GPU!\n");
            }
            
            int main()
            {
                    cuda_hello<<<1,1>>>();
                    cudaDeviceSynchronize();                                                                           //由于__global__形式的函数是异步的，因此需要同步一下（除此之外需要这个函数才可以将printf显示到屏幕上）。
                                                                                                                                                    //Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because 
                                                                                                                                                    //CUDA kernel launches don’t block the calling CPU thread). To do this I just call cudaDeviceSynchronize() 
                                                                                                                                                    //before doing the final error checking on the CPU.
                    return 0;
            }
    1.5. 在colab测试程序:
                ------------------------------------------------------------------------------------------
                //一个block，一个thread：
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          for (int i = 0; i < n; i++)
                            y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                //一个block，多个thread：
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          int index = threadIdx.x;
                          int stride = blockDim.x;
                          for (int i = index; i < n; i += stride)
                              y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                //多个block，多个thread：                
                        __global__
                        void add(int n, float *x, float *y)
                        {
                          int index = blockIdx.x * blockDim.x + threadIdx.x;
                          int stride = blockDim.x * gridDim.x;
                          for (int i = index; i < n; i += stride)
                            y[i] = x[i] + y[i];
                        }
                ------------------------------------------------------------------------------------------
                 //主函数
                 int main(void)
                {
                  int N = 1<<20;
                  float *x, *y;

                  cudaMallocManaged(&x, N*sizeof(float));
                  cudaMallocManaged(&y, N*sizeof(float));

                  for (int i = 0; i < N; i++) {
                    x[i] = 1.0f;
                    y[i] = 2.0f;
                  }
                ------------------------------------------------------------------------------------------
                //一个block，一个thread
                 add<<<1, 1>>>(N, x, y); 
                ------------------------------------------------------------------------------------------
                 //一个block，多个thread
                 add<<<1, 256>>>(N, x, y); 
                ------------------------------------------------------------------------------------------
                //多个block，多个thread
                int blockDim=256; 
                int gridNum=(N+blockDim-1)/blockDim;
                add<<<gridNum, blockDim>>>(N, x, y);
                ------------------------------------------------------------------------------------------
                  cudaDeviceSynchronize();

                  float maxError = 0.0f;
                  for (int i = 0; i < N; i++)
                    maxError = fmax(maxError, fabs(y[i]-3.0f));
                  std::cout << "Max error: " << maxError << std::endl;

                  cudaFree(x);
                  cudaFree(y);

                  return 0;
                }
    1.6. At its core are three key abstractions 
                - a hierarchy of thread groups, 
                - shared memories, 
                - barrier synchronization 
            that are simply exposed to the programmer as a minimal set of language extensions.
            These abstractions provide：
                - fine-grained data parallelism，
                - thread parallelism, 
                - nested within coarse-grained data parallelism，
                - task parallelism.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.07
参考链接：
        1. https://zhuanlan.zhihu.com/p/360897341，CUDA 编程小练习（一），使用colab进行cuda学习。
        2. https://zhuanlan.zhihu.com/p/361180950，CUDA 编程小练习（二），向量的加法。
        3. https://zhuanlan.zhihu.com/p/361467248，CUDA 编程小练习（三），向量的内积（单线程）。
        4. https://zhuanlan.zhihu.com/p/361997521，CUDA 编程小练习（四），矩阵加法。
        5. https://zhuanlan.zhihu.com/p/362796754，CUDA 编程小练习（五），使用nvprof来测试并优化程序。
        6. https://zhuanlan.zhihu.com/p/363820880，CUDA 编程小练习（六），多线程块向量加法。
        7. https://zhuanlan.zhihu.com/p/366196428，CUDA 编程小练习（七），单block多线程向量乘法。
        8. 
1. 简单程序练习
    此部分代码的书写逻辑均按照：
            (1) 分配cpu内存：e.g.  (float*)malloc(sizeof(float)*vectorLength);
            (2) 初始化被运算数据；
            (3) 分配gpu内存：e.g. cudaMalloc((void**)&x, sizeof(float)*length);
            (4) 拷贝运算数据从cpu内存到gpu内存，e.g.  cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);
            (5) gpu运算（使用kernel函数）；
            (6) 从gpu内存中将计算结果提取出来转移到cpu内存中，cudaMemcpy(result, d_result, sizeof(float),  cudaMemcpyDeviceToHost);
            (7) 对运算结果的数据处理；
            (8) 释放cpu和gpu用到的内存。
    1.1. 向量加法。
    1.2. 向量内积（单线程）。
    1.3. 矩阵加法。
    1.4. 使用nvprof来测试可执行程序的运行效率，并可根据结果优化程序。
    1.5. 使用单block多线程计算向量内积。
                解决思路：
                        1. 单block多线程算乘法，然后单线程算加法。
                        2. 单block多线程算乘法和本线程内的加法，然后单线程求和。
                - 例子程序复现没有问题，新增知识点：
                        extern __shared__ float temp[];  //  定义了一块block内，线程之间可以共享的内存块，其长度在调用CUDA函数的时候会被动态划分，也正因此，
                                                                                                上面这个函数可以实现任意长度的向量内积；缺点是temp的长度与输入向量长度相等，如果GPU内同时有大
                                                                                                量的向量在做点乘，就会比较耗费显存；
                        __syncthreads(); //  用于线程间的同步，在做累加之前需要确保每个线程都已经完成了乘法。                                                                      
                - 在改写过程中出现问题，我使用cudaMallocManaged()函数替代了原程序的内存操作步骤，但运算结果一直为0（前面改动后也一直为0），原因待查。
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.08
参考链接：
        1. https://zhuanlan.zhihu.com/p/366196428，CUDA 编程小练习（七），向量内积。
        2. https://zhuanlan.zhihu.com/p/367833053，CUDA 编程小练习（八），矩阵乘法。
        3. https://developer.nvidia.com/tensorrt-getting-started，TensorRT Getting Started。
        4. https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt-updated/, Speeding Up Deep Learning Inference Using NVIDIA TensorRT (Updated)。
        
    1.1. （问题尚未解决）在colab继续昨天的问题分析，问题查询思路：
                        - 根据手动计算，确定运算节点的位置并输出当前节点的计算结果，以此来判断改写出错在哪里。
                            - __global__ 函数中的中间参数无法打印输出，原因待分析。
                            - 经测试，使用cudaMallocManaged()改写后的数据中（一共有result, v1, v2, vLength)，v1和v2的数据是对的，但result的数据是不符合预期的（依然为0）。
                            - 重写了一边进行逻辑上查验，还是没有发现问题。
    1.2. 参考2的函数：（应该有优化空间，例如temp[]）
            __global__ void matrix_vector_multiplication(float* vector_result, float *matrix_a, float *vector_b, int m_row, int n_col) 
            {
                extern __shared__ float temp[]; 
                int tid = blockIdx.x * blockDim.x + threadIdx.x;

                // 上面这一生成索引会有tid超出矩阵边界的风险，因此需要用下面的条件语句加限制
                int size_of_the_matrix = m_row*n_col;
                if(tid<size_of_the_matrix)
                {
                    temp[tid] = matrix_a[tid] * vector_b[threadIdx.x]; // 所有的乘法，近乎同时完成。
                }

                __syncthreads(); // synchronize all threads

                if (threadIdx.x == 0)
                {
                    float sum = 0;
                    int index = blockIdx.x * blockDim.x;
                    for (int i = index; i < index + blockDim.x ; i++)
                    {
                        sum += temp[i];
                    }
                    vector_result[blockIdx.x] = sum;
                }
        }
2. TensorRT基本概念
    2.1.  NVIDIA TensorRT是深度学习的推理SDK（Software Development Kit，软件开发工具包）。
            - TensorRT provides APIs and parsers to import trained models from all major deep learning frameworks.
            - It then generates optimized runtime engines deployable in the datacenter as well as in automotive and embedded environments.
    2.2. 介绍的例子：
            概述：It uses a C++ example to walk you through converting a PyTorch model into an ONNX model and importing it into TensorRT, 
                         applying optimizations, and generating a high-performance runtime engine for the datacenter environment.
            具体任务：
                - deploy a deep learning application onto a GPU;
                - increasing throughput;
                - reducing latency during inference.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.11
参考链接：    
        1. https://zhuanlan.zhihu.com/p/367833824，CUDA 编程小练习（九），矩阵相乘的实现。
        2. https://blog.csdn.net/qq_42820594/article/details/82735387，C语言中，数字后面带个U，L，F的含义。
        3. https://www.zhihu.com/question/437131193/answer/1720580312，写CUDA到底难在哪？（综述性文章，来自知乎）
        4. http://www.mat.unimi.it/users/sansotte/cuda/CUDA_by_Example.pdf，CUDA By Example: An Introduction to General-Purpose GPU Programming。
        5. https://www.nvidia.cn/docs/IO/51635/NVIDIA_CUDA_Programming_Guide_1.1_chs.pdf，NVIDIA CUDA 编程指南。
        6. https://blog.csdn.net/chengde6896383/article/details/81009053，把书《CUDA By Example an Introduction to General Purpose GPU Programming》读薄。

1. 解决昨天遗留问题
    1.1. 遗留问题概述：将向量积cuda程序从手动分配内存改为全自动内存分配后出现计算结果不正确的情况。
             问题有2，分别是：
                    - 一是使用cudaMallocManaged()函数后没有使用cudaDeviceSynchronize()来进行同步，导致输出结果不正确。
                    - 二是在调用核函数时<<<...>>>这部分参数填写有问题。在成功运行的程序中，此部分参数输入和之前的教程不一样，需要日后查询原因。
             新问题：
                    - <<<...>>>这部分参数和之前的教程不一样，需要日后查询原因。
                    
  2. 编写矩阵乘法cuda的kernel函数
    2.1. 目前状况：kernel函数逻辑已确定，程序还在编写和调试（尚未测试）。
    2.2. 明日计划：完成函数编写，并进行调试和尝试优化。
 
 3. 进一步查询教程和资料
    3.1. 查询到新的教程：<<CUDA By Example: An Introduction to General-Purpose GPU Programming>>。
             进度：刚开始跟着学习。
    3.2. 阅读网上的cuda学习方法：
             https://www.zhihu.com/question/437131193/answer/1720580312，写CUDA到底难在哪？
             
    3.3. 阅读《把书《CUDA By Example an Introduction to General Purpose GPU Programming》读薄》
            https://blog.csdn.net/chengde6896383/article/details/81009053
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#2022.07.12
参考链接：
        1. https://blog.csdn.net/weixin_40294176/article/details/121280157?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-121280157-blog-80188045.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-121280157-blog-80188045.pc_relevant_aa&utm_relevant_index=1
             ，CUDA中Unified Memory函数cudaMallocManaged的使用。
        2. https://zhuanlan.zhihu.com/p/367833824，CUDA 编程小练习（九），矩阵乘法（一维grid）。
            https://github.com/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_09.ipynb。
        3. https://zhuanlan.zhihu.com/p/35004208，C语言数组的多种赋值方式。
        4. https://zhuanlan.zhihu.com/p/34587739，矩阵乘法，方法二（多维grid）。
        5. 

1. 矩阵乘法
    1.1. 方法一，每个线程计算一个向量积，使用1维grid。
            设有矩阵A，B，C，其中A，B为参与乘法计算的矩阵，C为结果矩阵。
            - A的维度为[ I x J ]，B的维度为[J x K]，根据乘法原则C的维度为[I x K]。
            - 计算过程：
            # 一个线程算一个向量，自动分配内存
                %%writefile Matrix_autoMalloc.cu
                #include <stdio.h>

                __global__ void matrix_mul(int *matrix_a, int *matrix_b, int *matrix_c,int matrix_a_row,int matrix_a_column,int matrix_b_column){
                    int matrix_c_element = 0;
                    for (int i = 0; i < matrix_a_column; i++){
                      matrix_c_element += matrix_a[(threadIdx.x/matrix_b_column)*matrix_a_column+i] * 
                                          matrix_b[threadIdx.x%matrix_b_column+i*matrix_b_column];
                    }
                    matrix_c[threadIdx.x]= matrix_c_element;
                }

                int main(int argc, char *argv[]){

                    int matrix_a_row = 4; //4
                    int matrix_a_column = 4;//4
                    int matrix_b_row = 4;//4
                    int matrix_b_column = 4;//4

                    int *matrix_a = (int*)malloc(sizeof(int)*(matrix_a_row * matrix_a_column));
                    int *matrix_b = (int*)malloc(sizeof(int)*(matrix_b_row * matrix_b_column));
                    int *matrix_c = (int*)malloc(sizeof(int) * (matrix_a_row * matrix_b_column));

                    int nBytes_a = sizeof(int) * (matrix_a_row * matrix_a_column);
                    int nBytes_b = sizeof(int) * (matrix_b_row * matrix_b_column);
                    int nBytes_c = sizeof(int) * (matrix_a_row * matrix_b_column);

                    cudaMallocManaged((void**)&matrix_a, nBytes_a);
                    cudaMallocManaged((void**)&matrix_b, nBytes_b);
                    cudaMallocManaged((void**)&matrix_c, nBytes_c);

                    // Example 1
                    int matrix_a_ini[16] = {5,0,34,21,7,17,-12,28,8,-3,-3,-3,0,-3,5,9};
                    int matrix_b_ini[16] = {0,16,24,-90,-23,0,11,1,3,3,0,3,66,7,8,0};
                    memcpy(matrix_a, matrix_a_ini,sizeof(int)*16);
                    memcpy(matrix_b, matrix_b_ini,sizeof(int)*16);


                    // implement 100 times for getting average execution time
                    for(int i=0; i<100;i++)
                    {
                       matrix_mul<<<1,matrix_a_row * matrix_b_column>>>(matrix_a, matrix_b, 
                                        matrix_c, matrix_a_row, matrix_a_column, matrix_b_column);
                    }

                    cudaDeviceSynchronize();

                    // print matrix_c to check correction
                    for(int i = 0; i < matrix_a_row; i++){
                        for(int j = 0; j < matrix_b_column; j++){
                            int index = i * matrix_b_column +j;
                            printf("%d, ",matrix_c[index]);
                        }
                        printf("\n");
                    }

                    cudaFree(matrix_c);
                    cudaFree(matrix_b);
                    cudaFree(matrix_a);

                    return 0;
                }
            
    1.2. 方法二：每个线程计算一个结果矩阵的元素，使用多维grid。
           %%writefile Matrix_autoMalloc_2.cu
                #include <iostream>

                // 矩阵类型，行优先，M(row, col) = *(M.elements + row * M.width + col)
                struct Matrix
                {
                    int width;
                    int height;
                    float *elements;
                };

                // 获取矩阵A的(row, col)元素
                __device__ float getElement(Matrix *A, int row, int col)
                {
                        return A->elements[row * A->width + col];
                }

                // 为矩阵A的(row, col)元素赋值
                __device__ void setElement(Matrix *A, int row, int col, float value)
                {
                        A->elements[row * A->width + col] = value;
                }

                // 矩阵相乘kernel，2-D，每个线程计算一个元素
                __global__ void matMulKernel(Matrix *A, Matrix *B, Matrix *C)
                {
                        float Cvalue = 0.0;
                        int row = threadIdx.y + blockIdx.y * blockDim.y;
                        int col = threadIdx.x + blockIdx.x * blockDim.x;
                        for (int i = 0; i < A->width; ++i)
                        {
                                Cvalue += getElement(A, row, i) * getElement(B, i, col);
                        }
                        setElement(C, row, col, Cvalue);
                }

                int main()
                {
                    int width = 32;//1 << 10;
                    int height = 32;//1 << 10;
                    Matrix *A, *B, *C;
                    // 申请托管内存
                    cudaMallocManaged((void**)&A, sizeof(Matrix));
                    cudaMallocManaged((void**)&B, sizeof(Matrix));
                    cudaMallocManaged((void**)&C, sizeof(Matrix));
                    int nBytes = width * height * sizeof(float);
                    cudaMallocManaged((void**)&A->elements, nBytes);
                    cudaMallocManaged((void**)&B->elements, nBytes);
                    cudaMallocManaged((void**)&C->elements, nBytes);

                    // 初始化数据
                    A->height = height;
                    A->width = width;
                    B->height = height;
                    B->width = width;
                    C->height = height;
                    C->width = width;
                    for (int i = 0; i < width * height; ++i)
                    {
                        A->elements[i] = 1.0;
                        B->elements[i] = 2.0;
                    }

                    // 定义kernel的执行配置
                    dim3 blockSize(32, 32);
                    dim3 gridSize(1,1);/*gridSize((width + blockSize.x - 1) / blockSize.x, 
                        (height + blockSize.y - 1) / blockSize.y);*/
                    // 执行kernel
                    matMulKernel << <gridSize, blockSize >> >(A, B, C);//gridSize


                    // 同步device 保证结果能正确访问
                    cudaDeviceSynchronize();
                    // 检查执行结果
                    float maxError = 0.0;
                    for (int i = 0; i < width * height; ++i)
                        maxError = fmax(maxError, fabs(C->elements[i] - 2 * width));
                    std::cout << "最大误差: " << maxError << std::endl;

                    // print C to check correction
                /*    for(int i = 0; i < height; i++){
                        for(int j = 0; j < width; j++){
                            int index = i * width +j;
                            printf("%f, ",C->elements[index]);
                        }
                        printf("\n");
                    }*/
                    return 0;
                }
        
            
2.  关于cudaMallocManaged()的用法查询
        - Unified Memory的使用主要就是简化代码，不需要类似于cudamalloc和cudamemcpy。
        - cudaMallocManaged(void **devPtr, size_t size, unsigned int flags __dv(cudaMemAttachGlobal)); 一般传前两个参数即可。
        - 传指针时先在C代码中malloc，然后cudaMallocManaged()，再赋值，否则cudaMallocManaged()会将值重置为0，结构体内指针重置为00000000。
        - 带指针的结构体：
                一句话总结：结构体指针先cudaMallocManaged，
                                           结构体内指针后cudaMallocManaged，然后再给结构体内指针赋值。结构体内其他变量不受影响。

3. C语言数组的多种赋值方式
        - 首先是数组的声明，数组在声明的时候可以连续进行赋值；
                int a[3] = {1,2,3};   //可行
        - 但进行声明后就不可以进行多元素的赋值（不包括memcpy），只能对每个元素进行赋值；
                int a[3];
                a[3] = {1,2,3}; //不可行，不符合语法规范，
                        - 这时只能进行元素的单独赋值：a[0] = 1; a[1] = 2; a[2] = 3; 
                        - 或者使用循环进行变量赋值：int a[3];
                                                                                     for(int i = 0; i < 3; i++)
                                                                                        a[i] = i + 1;
        - 在初始化赋值中，静态数组可以直接使用int a[3] = {0};进行初始化赋值，这里还有另外一种方法就是使用memset函数进行初始化操作，
            - memset的函数原型为：void *memset(void *s, int ch, size_t n); 
            - 将s中当前位置后面的n个字节 （typedef unsigned int size_t ）用 ch 替换并返回 s 。（注意：这里是按字节进行替换的，所以int型
                不能设置成别的数，因为int是4个字节，而设置是一个字节）
            - memset函数的使用如下：int a[3]; memset(a,0,sizeof(int)*3); 此种赋值方式与声明时进行赋值的不同可以体现在动态数组中，因为
                动态数组不能在声明时进行初始化，而memset却可以对动态数组进行初始化，对动态数组的初始化如下：
                int *a;
                a = (int*)malloc(sizeof(int)*5);
                memset(a,0,sizeof(int)*5);
        - 使用memcpy函数，memcpy的函数原型为 ：void *memcpy(void *dest, const void *src, size_t n);






 
